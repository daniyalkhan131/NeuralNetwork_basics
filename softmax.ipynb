{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J1_9Y1je8LIp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "#from IPython.display import display, Markdown, Latex\n",
        "#from sklearn.datasets import make_blobs\n",
        "#from matplotlib.widgets import Slider"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_softmax(z):\n",
        "    ez = np.exp(z)              #element-wise exponenial\n",
        "    sm = ez/np.sum(ez)\n",
        "    return(sm)"
      ],
      "metadata": {
        "id": "EMzlc_fF9oT3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make  dataset for example\n",
        "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
        "#X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)\n",
        "X_train=np.array([[-5, 2], [-2, -2], [1, 2], [5, -2]])\n",
        "y_train=np.array([0,1,2,3]).reshape(-1,1)     # remember label shoul start from 0 as it must be < size of len of it"
      ],
      "metadata": {
        "id": "tvfqO19VKCQU"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.tile(X_train,(100,1))\n",
        "#X=X_train\n",
        "y=np.tile(y_train,(100,1))\n",
        "y=y[:,0]"
      ],
      "metadata": {
        "id": "Tdv8MB4KKF6t"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"The Obvious organization¶\n",
        "The model below is implemented with the softmax as an activation in the final Dense layer. The loss function is separately specified in the compile directive.\n",
        "\n",
        "The loss function is SparseCategoricalCrossentropy. This loss is described in (3) above. In this model, the softmax takes place in the last layer. \n",
        "The loss function takes in the softmax output which is a vector of probabilities.\"\"\"\n",
        "\n",
        "print(X[:9])\n",
        "print(y[:9])\n",
        "print(len(X),len(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vItaK2qJ4zna",
        "outputId": "5de7a968-2a1b-4c43-e39c-e714918c6cc1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-5  2]\n",
            " [-2 -2]\n",
            " [ 1  2]\n",
            " [ 5 -2]\n",
            " [-5  2]\n",
            " [-2 -2]\n",
            " [ 1  2]\n",
            " [ 5 -2]\n",
            " [-5  2]]\n",
            "[0 1 2 3 0 1 2 3 0]\n",
            "400 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential(\n",
        "    [ \n",
        "        Dense(25, activation = 'relu'),\n",
        "        Dense(15, activation = 'relu'),\n",
        "        Dense(4, activation = 'softmax')    # < softmax activation here\n",
        "    ]\n",
        ")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X,y,epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZKl4L4r40b7",
        "outputId": "cdc4cca6-c513-47cb-9595-26d7ca334ecb"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 1s 2ms/step - loss: 1.5702\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.2503\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.0372\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 0.8841\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.7736\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 0.6845\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 0.6072\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 0.5447\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 0.4896\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 0.4415\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6b0583ff10>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Because the softmax is integrated into the output layer, the output is a vector of probabilities."
      ],
      "metadata": {
        "id": "s8gLohSP53vS"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_nonpreferred = model.predict(X_train)\n",
        "print(p_nonpreferred [:5])\n",
        "print(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kagITvO69bul",
        "outputId": "2e10c71c-26be-4919-a909-09c7bf6c009a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 95ms/step\n",
            "[[7.6743430e-01 1.8939564e-01 2.7855283e-02 1.5314807e-02]\n",
            " [3.2058734e-02 9.0862471e-01 4.1922186e-02 1.7394384e-02]\n",
            " [3.1962457e-01 2.5320107e-01 2.8336248e-01 1.4381191e-01]\n",
            " [7.8009553e-03 3.6172342e-02 4.0040983e-04 9.5562631e-01]]\n",
            "largest value 0.9556263 smallest value 0.00040040983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p=np.arange(1,21).reshape(5,-1)\n",
        "print(p)\n",
        "print(np.max(p[0,:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRV2vmJiAPgp",
        "outputId": "be224310-e483-4a39-d969-8393e14a271b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  2  3  4]\n",
            " [ 5  6  7  8]\n",
            " [ 9 10 11 12]\n",
            " [13 14 15 16]\n",
            " [17 18 19 20]]\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Preferred \n",
        "Recall from lecture, more stable and accurate results can be obtained if the softmax and loss are combined during training.\n",
        " This is enabled by the 'preferred' organization shown here.\n",
        "\n",
        "In the preferred organization the final layer has a linear activation. For historical reasons, the outputs in this form are referred to as logits. \n",
        "The loss function has an additional argument: from_logits = True. This informs the loss function that the softmax operation should be included in the loss calculation. \n",
        "This allows for an optimized implementation.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "gKI413-ZAXF2",
        "outputId": "303e2ed7-1a63-4ef4-9fd7-af0e5be9971f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Preferred \\nRecall from lecture, more stable and accurate results can be obtained if the softmax and loss are combined during training.\\n This is enabled by the 'preferred' organization shown here.\\n\\nIn the preferred organization the final layer has a linear activation. For historical reasons, the outputs in this form are referred to as logits. \\nThe loss function has an additional argument: from_logits = True. This informs the loss function that the softmax operation should be included in the loss calculation. \\nThis allows for an optimized implementation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preferred_model = Sequential(\n",
        "    [ \n",
        "        Dense(25, activation = 'relu'),\n",
        "        Dense(15, activation = 'relu'),\n",
        "        Dense(4, activation = 'linear')   #<-- Note\n",
        "    ]\n",
        ")\n",
        "preferred_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        ")\n",
        "\n",
        "preferred_model.fit(\n",
        "    X_train,y_train,\n",
        "    epochs=10\n",
        ")\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI1bwmVrBBHe",
        "outputId": "b6613bb8-df16-4cd4-9a69-11ea8e26344e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 1s 800ms/step - loss: 1.7217\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.6799\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6388\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.5985\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.5589\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5226\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4885\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4566\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4288\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4013\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6b0bfca800>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Notice that in the preferred model, the outputs are not probabilities, but can range from large negative numbers to large positive numbers. The output must be sent through a softmax when performing a prediction that expects a probability.\n",
        "# Let's look at the preferred model outputs:"
      ],
      "metadata": {
        "id": "6z180CkWBFgl"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_preferred = preferred_model.predict(X)\n",
        "print(f\"two example output vectors:\\n {p_preferred[:9]}\")\n",
        "print(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTkrPiXHBOi0",
        "outputId": "fa68fbf7-ea89-4f8c-ac1a-98f75ff43a76"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 3ms/step\n",
            "two example output vectors:\n",
            " [[ 0.50100267  0.16643001 -0.3375221  -0.38199842]\n",
            " [-0.2661833  -0.94846386 -0.58020353  0.37000045]\n",
            " [ 0.01260066  0.10695031  0.16173512 -0.2072627 ]\n",
            " [ 0.9196835  -0.12825637 -0.29331413  0.5713747 ]\n",
            " [ 0.50100267  0.16643001 -0.3375221  -0.38199842]\n",
            " [-0.2661833  -0.94846386 -0.58020353  0.37000045]\n",
            " [ 0.01260066  0.10695031  0.16173512 -0.2072627 ]\n",
            " [ 0.9196835  -0.12825637 -0.29331413  0.5713747 ]\n",
            " [ 0.50100267  0.16643001 -0.3375221  -0.38199842]]\n",
            "largest value 0.91968364 smallest value -0.94846386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sm_preferred = tf.nn.softmax(p_preferred).numpy() #.numpy() for converting tensorflow rep to numpy rep\n",
        "print(f\"two example output vectors:\\n {sm_preferred[:2]}\")\n",
        "print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMpbSW2lBQWf",
        "outputId": "87840168-8b8a-4dd7-d920-03a7f0ae6680"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "two example output vectors:\n",
            " [[0.39039144 0.27938122 0.1687849  0.16144247]\n",
            " [0.24241106 0.1225298  0.1770823  0.45797685]]\n",
            "largest value 0.45797685 smallest value 0.122529805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sm_preferred = list()\n",
        "for i in p_preferred:\n",
        "  sm_preferred.append(my_softmax(i))\n",
        "sm_preferred=np.array(sm_preferred)\n",
        "print(f\"two example output vectors:\\n {sm_preferred[:2]}\")\n",
        "print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsde2vBWBfeY",
        "outputId": "cabb3a32-360b-4246-817b-a9bdd87114a7"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "two example output vectors:\n",
            " [[0.3903914  0.27938125 0.16878492 0.16144247]\n",
            " [0.24241108 0.1225298  0.17708232 0.4579768 ]]\n",
            "largest value 0.4579768 smallest value 0.1225298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To select the most likely category, the softmax is not required. One can find the index of the largest output using np.argmax().\n",
        "\n",
        "p=np.random.rand(5)\n",
        "print(p)\n",
        "print(np.argmax(p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7FciVx9Bphv",
        "outputId": "20d5d2b5-2fd0-43a3-8b7b-44c3b1132b1f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.21759378 0.78541515 0.95661636 0.17922873 0.82137044]\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print( f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0je2nONjC6ul",
        "outputId": "2baaa440-24b6-4c4c-8f69-b7255b8f382f"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.50100267  0.16643001 -0.3375221  -0.38199842], category: 0\n",
            "[-0.2661833  -0.94846386 -0.58020353  0.37000045], category: 3\n",
            "[ 0.01260066  0.10695031  0.16173512 -0.2072627 ], category: 2\n",
            "[ 0.9196835  -0.12825637 -0.29331413  0.5713747 ], category: 0\n",
            "[ 0.50100267  0.16643001 -0.3375221  -0.38199842], category: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"SparseCategorialCrossentropy or CategoricalCrossEntropy¶\n",
        "Tensorflow has two potential formats for target values and the selection of the loss defines which is expected.\n",
        "\n",
        "SparseCategorialCrossentropy: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9.\n",
        "CategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded where the value at the target index is 1 while the other N-1 entries are zero.\n",
        " An example with 10 potential target values, where the target is 2 would be [0,0,1,0,0,0,0,0,0,0].\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "YPuXdEP3C-7N",
        "outputId": "d0247457-f744-495e-d526-71be4bddd5e7"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SparseCategorialCrossentropy or CategoricalCrossEntropy¶\\nTensorflow has two potential formats for target values and the selection of the loss defines which is expected.\\n\\nSparseCategorialCrossentropy: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9.\\nCategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded where the value at the target index is 1 while the other N-1 entries are zero.\\n An example with 10 potential target values, where the target is 2 would be [0,0,1,0,0,0,0,0,0,0].'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n0nGu6X6EC4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}