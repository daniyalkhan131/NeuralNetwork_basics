{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rKVkHSTVJCtB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "#import logging\n",
        "#logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "#tf.autograph.set_verbosity(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array([[1,3],[2,7],[3,6],[2,4],[6,5],[7,6]])\n",
        "Y=np.array([[1],[0],[1],[0],[1],[1]])\n",
        "print(X.shape, Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dsAPM9UJZSs",
        "outputId": "39d4dea1-acaa-4dc9-ec2b-067922768b5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 2) (6, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Normalize Data\n",
        "Fitting the weights to the data (back-propagation, covered in next week's lectures) will proceed more quickly if the data is normalized.\n",
        " This is the same procedure you used in Course 1 where features in the data are each normalized to have a similar range. The procedure below uses a Keras normalization layer.\n",
        "  It has the following steps:\n",
        "\n",
        "create a \"Normalization Layer\". Note, as applied here, this is not a layer in your model.\n",
        "'adapt' the data. This learns the mean and variance of the data set and saves the values internally.\n",
        "normalize the data.\n",
        "It is important to apply normalization to any future data that utilizes the learned model.\"\"\"\n",
        "\n",
        "\n",
        "print(f\"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}\")\n",
        "print(f\"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}\")\n",
        "norm_l = tf.keras.layers.Normalization(axis=-1)\n",
        "norm_l.adapt(X)  # learns mean, variance\n",
        "Xn = norm_l(X)\n",
        "print(f\"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}\")\n",
        "print(f\"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF1nomgUJy9B",
        "outputId": "15d77f04-1957-4772-98d3-5da00b69aba5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature Max, Min pre normalization: 7.00, 1.00\n",
            "Duration    Max, Min pre normalization: 7.00, 3.00\n",
            "Temperature Max, Min post normalization: 1.58, -1.13\n",
            "Duration    Max, Min post normalization: 1.36, -1.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tile/copy our data to increase the training set size and reduce the number of training epochs\n",
        "\n",
        "Xt = np.tile(Xn,(10000,1))          #1000 is no. of repetation in row and 1 is no. of repetation in column\n",
        "Yt= np.tile(Y,(10000,1))   \n",
        "print(Xt.shape, Yt.shape)   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyIXGTYzLd8J",
        "outputId": "64f14955-6aa6-4e90-d96b-44f5f01aa725"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 2) (60000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(2,)),\n",
        "        Dense(3, activation='sigmoid', name = 'layer1'),\n",
        "        Dense(1, activation='sigmoid', name = 'layer2')\n",
        "     ]\n",
        ")"
      ],
      "metadata": {
        "id": "Th6fCJFELkli"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Note 1: The tf.keras.Input(shape=(2,)), specifies the expected shape of the input. This allows Tensorflow to size the weights and bias parameters at this point.\n",
        " This is useful when exploring Tensorflow models. This statement can be omitted in practice and Tensorflow will size the network parameters when the input data is \n",
        " \n",
        " specified in the model.fit statement.\n",
        "Note 2: Including the sigmoid activation in the final layer is not considered best practice. It would instead be accounted for in the loss which improves numerical stability.\n",
        " This will be described in more detail in a later lab.\n",
        "\n",
        "The model.summary() provides a description of the network:\"\"\"\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLpxSiNVLyIT",
        "outputId": "773eedee-d769-4514-9c59-ea2fab71a2a8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " layer1 (Dense)              (None, 3)                 9         \n",
            "                                                                 \n",
            " layer2 (Dense)              (None, 1)                 4         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13\n",
            "Trainable params: 13\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "L1_num_params = 2 * 3 + 3   # W1 parameters  + b1 parameters\n",
        "L2_num_params = 3 * 1 + 1   # W2 parameters  + b2 parameters\n",
        "print(\"L1 params = \", L1_num_params, \", L2 params = \", L2_num_params  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUEY2snzL6xi",
        "outputId": "f689ceb7-c751-41b2-e5ef-89468c78f6b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 params =  9 , L2 params =  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Let's examine the weights and biases Tensorflow has instantiated. The weights  ð‘Š\n",
        "  should be of size (number of features in input, number of units in the layer) while the bias  ð‘\n",
        "  size should match the number of units in the layer:\n",
        "\n",
        "In the first layer with 3 units, we expect W to have a size of (2,3) and  ð‘\n",
        "  should have 3 elements.\n",
        "In the second layer with 1 unit, we expect W to have a size of (3,1) and  ð‘\n",
        "  should have 1 element\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "K1o43DsOMK5o",
        "outputId": "d4762fbd-971e-48e2-a77a-d15654fe7814"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Let's examine the weights and biases Tensorflow has instantiated. The weights  ð‘Š\\n  should be of size (number of features in input, number of units in the layer) while the bias  ð‘\\n  size should match the number of units in the layer:\\n\\nIn the first layer with 3 units, we expect W to have a size of (2,3) and  ð‘\\n  should have 3 elements.\\nIn the second layer with 1 unit, we expect W to have a size of (3,1) and  ð‘\\n  should have 1 element\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
        "print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)"
      ],
      "metadata": {
        "id": "3qXzxYzWMPsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34feea72-9015-4ebd-b493-62f62f936568"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1(2, 3):\n",
            " [[ 0.05938005 -0.6377648  -0.72364426]\n",
            " [-0.5575681  -0.08160245 -0.14975691]] \n",
            "b1(3,): [0. 0. 0.]\n",
            "W2(3, 1):\n",
            " [[-0.3858496 ]\n",
            " [-0.07881188]\n",
            " [ 1.0169193 ]] \n",
            "b2(1,): [0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    Xt,Yt,            \n",
        "    epochs=10,\n",
        ")\n",
        "\n",
        "\n",
        "\"\"\"Epochs and batches\n",
        "In the compile statement above, the number of epochs was set to 10. This specifies that \n",
        "the entire data set should be applied during training 10 times. During training, you see output describing the progress of training that looks like this:\n",
        "\n",
        "Epoch 1/10\n",
        "6250/6250 [==============================] - 6s 910us/step - loss: 0.1782\n",
        "The first line, Epoch 1/10, describes which epoch the model is currently running. For efficiency, \n",
        "the training data set is broken into 'batches'. The default size of a batch in Tensorflow is 32. There are\n",
        " 200000 examples in our expanded data set or 6250 batches. The notation on the 2nd line 6250/6250 [==== is describing which batch has been executed.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "AfOI_0aZM7TJ",
        "outputId": "50d452f0-9cf0-4d7f-e595-1188084bfeed"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 2s 992us/step - loss: 0.3040\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 2s 983us/step - loss: 0.0873\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 2s 993us/step - loss: 0.0119\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0037\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0014\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 2s 985us/step - loss: 5.2598e-04\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 2s 1ms/step - loss: 2.0820e-04\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 2s 988us/step - loss: 8.3055e-05\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 2s 956us/step - loss: 3.3281e-05\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 2s 980us/step - loss: 1.3361e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Epochs and batches\\nIn the compile statement above, the number of epochs was set to 10. This specifies that \\nthe entire data set should be applied during training 10 times. During training, you see output describing the progress of training that looks like this:\\n\\nEpoch 1/10\\n6250/6250 [==============================] - 6s 910us/step - loss: 0.1782\\nThe first line, Epoch 1/10, describes which epoch the model is currently running. For efficiency, \\nthe training data set is broken into 'batches'. The default size of a batch in Tensorflow is 32. There are\\n 200000 examples in our expanded data set or 6250 batches. The notation on the 2nd line 6250/6250 [==== is describing which batch has been executed.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
        "print(\"W2:\\n\", W2, \"\\nb2:\", b2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QudplNhNPdQ",
        "outputId": "c454a027-caea-49ce-f869-a9e82bfcf50a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1:\n",
            " [[  3.0724306 -21.91437     3.723444 ]\n",
            " [  9.87312    -2.464961   -2.2128153]] \n",
            "b1: [15.111537  -8.899456   6.3772655]\n",
            "W2:\n",
            " [[-22.683119]\n",
            " [-23.287506]\n",
            " [ 19.798985]] \n",
            "b2: [15.174013]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Next, we will load some saved weights from a previous training run. This is so that this notebook remains robust to changes in\n",
        " Tensorflow over time. Different training runs can produce somewhat different results and the discussion below applies to a particular solution. Feel free to re-run the\n",
        " notebook with this cell commented out to see the difference.\"\"\"\n",
        "\n",
        "W1 = np.array([\n",
        "    [-8.94,  0.29, 12.89],\n",
        "    [-0.17, -7.34, 10.79]] )\n",
        "b1 = np.array([-9.87, -9.28,  1.01])\n",
        "W2 = np.array([\n",
        "    [-31.38],\n",
        "    [-27.86],\n",
        "    [-32.79]])\n",
        "b2 = np.array([15.54])\n",
        "model.get_layer(\"layer1\").set_weights([W1,b1])\n",
        "model.get_layer(\"layer2\").set_weights([W2,b2])"
      ],
      "metadata": {
        "id": "Ws3k7ZEUN_l5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Once you have a trained model, you can then use it to make predictions. Recall that the output of our model is a probability. \n",
        "In this case, the probability of a good roast. To make a decision, one must apply the probability to a threshold. In this case, we will use 0.5\n",
        "\n",
        "Let's start by creating input data. The model is expecting one or more examples where examples are in the rows of matrix. \n",
        "In this case, we have two features so the matrix will be (m,2) where m is the number of examples. Recall, we have normalized \n",
        "the input features so we must normalize our test data as well.\n",
        "To make a prediction, you apply the predict method.\"\"\"\n",
        "\n",
        "\n",
        "X_test = np.array([\n",
        "    [200,13.9],  # postive example\n",
        "    [200,17]])   # negative example\n",
        "X_testn = norm_l(X_test)\n",
        "predictions = model.predict(X_testn)\n",
        "print(\"predictions = \\n\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_uudZuUOUE_",
        "outputId": "69d5e249-8258-485a-a44e-98f442cf328a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 60ms/step\n",
            "predictions = \n",
            " [[3.2241868e-08]\n",
            " [3.2241868e-08]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(type(predictions))\n",
        "yhat = np.zeros_like(predictions)\n",
        "for i in range(len(predictions)):\n",
        "    if predictions[i] >= 0.5:\n",
        "        yhat[i] = 1\n",
        "    else:\n",
        "        yhat[i] = 0\n",
        "print(f\"decisions = \\n{yhat}\")\n",
        "\n",
        "#or\n",
        "yhat = (predictions >= 0.5).astype(int)\n",
        "print(f\"decisions = \\n{yhat}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMzzA8DpOxAz",
        "outputId": "dd79ee76-2c13-4d71-a3d7-65ca5e37c1c0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "decisions = \n",
            "[[0.]\n",
            " [0.]]\n",
            "decisions = \n",
            "[[0]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jad_dqzdPJ4O"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mM_Fty-P4qJP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}